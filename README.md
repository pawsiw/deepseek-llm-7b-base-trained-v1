# Training: deepseek-llm-7b-base-trained-v1
# ğŸ§  deepseek-llm-7b-base-trained-v1

Fine-tuning and training setup for DeepSeek LLM 7B model using custom source code and RAG.

---

## ğŸ“ Project Directory Structure

The root directory on the local machine is `/mnt/deepseek/`. All project-related directories are organized under this root with a clear separation between models, data sources, training runs, and tools:

```
/mnt/deepseek/
â”œâ”€â”€ deepseek-llm-7b-base-trained-v1/  
â”‚   â”œâ”€â”€ models/                       
â”‚   â”‚   â””â”€â”€ <model_name>/             
â”‚   â”‚       â”œâ”€â”€ config/               # Model, tokenizer, and training configs
â”‚   â”‚       â”œâ”€â”€ checkpoints/          # Checkpoints from training
â”‚   â”‚       â”œâ”€â”€ logs/                 # Logs from training or evaluation
â”‚   â”‚       â”œâ”€â”€ scripts/              # Training/inference scripts (e.g. train_lora.py)
â”‚   â”‚       â””â”€â”€ README.md             # Documentation for the model
â”‚   â”‚
â”‚   â”œâ”€â”€ projects/                     
â”‚   â”‚   â””â”€â”€ <project_name>/           
â”‚   â”‚       â”œâ”€â”€ src/                  # Source code
â”‚   â”‚       â”œâ”€â”€ docs/                 # Documentation
â”‚   â”‚       â”œâ”€â”€ data/                 # Optional input data
â”‚   â”‚       â””â”€â”€ README.md
â”‚   â”‚
â”‚   â”œâ”€â”€ datasets/                     
â”‚   â”‚   â””â”€â”€ <dataset_name>/           # Processed datasets for training (JSONL, pickle, etc.)
â”‚   â”‚       â”œâ”€â”€ config.json           # Snapshot of run config
â”‚   â”‚       â”œâ”€â”€ raw/                  # Copied source code files
â”‚   â”‚       â”œâ”€â”€ processed/            # JSONL files for pretrain/instruct formats
â”‚   â”‚       â”‚   â”œâ”€â”€ pretrain.jsonl    # self-supervised code corpus (planned)
â”‚   â”‚       â”‚   â””â”€â”€ pretrain.jsonl    # instruct.jsonl`: instruction-tuned dataset (planned)
â”‚   â”‚       â”œâ”€â”€ meta.json
â”‚   â”‚       â””â”€â”€ README.md
â”‚   â”‚
â”‚   â”œâ”€â”€ runs/                         
â”‚   â”‚   â””â”€â”€ <run_id>/                 
â”‚   â”‚       â”œâ”€â”€ config.json           # Snapshot of run config
â”‚   â”‚       â”œâ”€â”€ metrics.json          # Evaluation metrics
â”‚   â”‚       â”œâ”€â”€ log.txt               # Execution log
â”‚   â”‚       â””â”€â”€ output/               # Output from the run
â”‚   â”‚
â”‚   â”œâ”€â”€ tools/                        
â”‚   â”‚   â””â”€â”€ tokenizer_utils.py, preprocess.py, etc.
â”‚   â”‚
â”‚   â””â”€â”€ shared/                       
â”‚       â””â”€â”€ prompts/, glossary.txt, ...
â”‚
â”œâ”€â”€ projects/my_agent/                # Own agent implementation
â”œâ”€â”€ projects/python-genai/            # Reference Gemini implementation (Google)
```

**Note:** The source files under `/mnt/deepseek/deepseek-llm-7b-base-trained-v1/` are stored in the GitHub repository: [https://github.com/pawsiw/deepseek-llm-7b-base-trained-v1.git](https://github.com/pawsiw/deepseek-llm-7b-base-trained-v1.git)

---

## ğŸ”§ Training Strategy

This model is trained using a retrieval-augmented generation (RAG) approach, leveraging isolated customer code repositories. The training pipeline includes:

- Data preprocessing from `projects/`
- Dataset generation into `datasets/`
- LoRA fine-tuning via scripts in `models/<name>/scripts/`
- Logging into `runs/`

---

## ğŸ“Œ Notes

- All documentation, comments, and instructions are written in **English**
- No code is auto-generated unless explicitly requested
- This model acts as a **code-generating assistant**, not a chatbot
- All actions must be logged and reproducible
- Git is used for versioning scripts and configurations

---

## ğŸ§© Dependencies

- Python 3.10+
- Transformers >= 4.38
- DeepSeek LLM Toolkit
- Optional: `accelerate`, `peft`, `bitsandbytes`, `datasets`

---

## ğŸ§ª Inference

To test inference locally:

```bash
cd models/deepseek-llm-7b-base-trained-v1/scripts/
python infer.py --model_dir ../checkpoints/last/ --prompt "Write a hello world script"
```

