# Training: deepseek-llm-7b-base-trained-v1
# ğŸ§  deepseek-llm-7b-base-trained-v1

Fine-tuning and training setup for DeepSeek LLM 7B model using custom source code and RAG.

---

## ğŸ“ Project Directory Structure

All project-related directories are organized under `/mnt/deepseek` with a clear separation between models, data sources, training runs, and tools:

/mnt/deepseek/
â”œâ”€â”€ models/             # All trained models and their configurations
â”‚   â””â”€â”€ <model_name>/   # e.g. deepseek-llm-7b-base-trained-v1
â”‚       â”œâ”€â”€ config/         # Model, tokenizer, and training configs
â”‚       â”œâ”€â”€ checkpoints/    # Checkpoints from training
â”‚       â”œâ”€â”€ logs/           # Logs from training or evaluation
â”‚       â”œâ”€â”€ scripts/        # Training/inference scripts (e.g. train_lora.py)
â”‚       â””â”€â”€ README.md       # Documentation for the model
â”‚
â”œâ”€â”€ projects/           # Source repositories and RAG inputs
â”‚   â””â”€â”€ <project_name>/     # e.g. my-agent, customers-api
â”‚       â”œâ”€â”€ src/            # Source code
â”‚       â”œâ”€â”€ docs/           # Documentation
â”‚       â”œâ”€â”€ data/           # Optional input data
â”‚       â””â”€â”€ README.md
â”‚
â”œâ”€â”€ datasets/           # Processed datasets for training (JSONL, pickle, etc.)
â”‚   â””â”€â”€ <dataset_name>.jsonl
â”‚
â”œâ”€â”€ runs/               # Individual training/evaluation runs
â”‚   â””â”€â”€ <run_id>/           # e.g. deepseek-v1-20250628
â”‚       â”œâ”€â”€ config.json     # Snapshot of run config
â”‚       â”œâ”€â”€ metrics.json    # Evaluation metrics
â”‚       â”œâ”€â”€ log.txt         # Execution log
â”‚       â””â”€â”€ output/         # Output from the run
â”‚
â”œâ”€â”€ tools/              # Utilities and helper scripts
â”‚   â””â”€â”€ tokenizer_utils.py, preprocess.py, etc.
â”‚
â””â”€â”€ shared/             # Optional shared assets (e.g. prompts, glossaries)
    â””â”€â”€ prompts/, glossary.txt, ...


---

## ğŸ”§ Training Strategy

This model is trained using a retrieval-augmented generation (RAG) approach, leveraging isolated customer code repositories. The training pipeline includes:

- Data preprocessing from `projects/`
- Dataset generation into `datasets/`
- LoRA fine-tuning via scripts in `models/<name>/scripts/`
- Logging into `runs/`

---

## ğŸ“Œ Notes

- All documentation, comments, and instructions are written in **English**
- No code is auto-generated unless explicitly requested
- This model acts as a **code-generating assistant**, not a chatbot
- All actions must be logged and reproducible
- Git is used for versioning scripts and configurations

---

## ğŸ§© Dependencies

- Python 3.10+
- Transformers >= 4.38
- DeepSeek LLM Toolkit
- Optional: `accelerate`, `peft`, `bitsandbytes`, `datasets`

---

## ğŸ§ª Inference

To test inference locally:

```bash
cd models/deepseek-llm-7b-base-trained-v1/scripts/
python infer.py --model_dir ../checkpoints/last/ --prompt "Write a hello world script"

